{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JBLeeDeepLearn_First_NeuralNet_WithKeras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athiagarajan/JBLeeFiles/blob/master/JBLeeDeepLearn_First_NeuralNet_WithKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtmMhqfsL9Bw"
      },
      "source": [
        "# **First Neural Network With Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74XwKafVMDeU"
      },
      "source": [
        "# Create your first MLP in Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "# Compile model\n",
        "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOht0CaHPl3U"
      },
      "source": [
        "# **Evaluating Performance of Deep Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o65_qHYVPtXG"
      },
      "source": [
        "# Create your first MLP in Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "# Compile model\n",
        "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)\n",
        "# evaluate the model\n",
        "#scores = model.evaluate(X, Y)\n",
        "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQiK9OAQR-9"
      },
      "source": [
        "# MLP with manual validation set\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# split into 67% for train and 33% for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "# Compile model\n",
        "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcWMVmtsRI5J"
      },
      "source": [
        "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(X, Y):\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "  # Compile model\n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "  # Fit the model\n",
        "  model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO8xI3dIbLDS"
      },
      "source": [
        "#**Keras Models With Scikit-Learn For General Machine Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_3B4YJCb3bS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec382a6-2bd0-4e8d-b9fa-696e08627028"
      },
      "source": [
        "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(1, kernel_initializer= 'uniform' , activation= 'sigmoid' ))\n",
        "  # Compile model\n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "  return model\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10, verbose=0)\n",
        "# evaluate using 10-fold cross validation\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6510594606399536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtrUea2QNx9M"
      },
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer= 'rmsprop' , init= 'glorot_uniform' ):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "init = [ 'glorot_uniform' , 'normal' , 'uniform' ]\n",
        "optimizers = [ 'rmsprop' , 'adam' ]\n",
        "#param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batch_size, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "pdeep = grid_result.get_params([True])\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r %r\" % (mean, stdev, param,pdeep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdv17LDBC9T"
      },
      "source": [
        "# **Multiclass Classification with the Iris Flowers Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ3wDt1eA7f1",
        "outputId": "cba8c790-661a-4b97-c776-14326a4c0152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# multi-class classification with Keras\r\n",
        "import pandas\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.utils import np_utils\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = pandas.read_csv(\"iris.data.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "X = dataset[:,0:4].astype(float)\r\n",
        "Y = dataset[:,4]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\r\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\r\n",
        " \r\n",
        "# define baseline model\r\n",
        "def baseline_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\r\n",
        "\tmodel.add(Dense(3, activation='softmax'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        " \r\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\r\n",
        "kfold = KFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0667029b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065effba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ecee840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065edf87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ec51730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065e93b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ea102f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065e8f7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ed46ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f06626ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 98.67% (2.67%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmhC1biRHcpV",
        "outputId": "248c6951-4608-4b9e-d50a-bde3e7359b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Binary Classification with Sonar Dataset: Baseline\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:60].astype(float)\r\n",
        "Y = dataset[:,60]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# baseline model\r\n",
        "def create_baseline():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\r\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "# evaluate model with standardized dataset\r\n",
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\r\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065b38e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 83.14% (5.86%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVTKnzwCIC-i",
        "outputId": "e816253f-121d-4c8b-c0cc-f4d31bbeddfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Binary Classification with Sonar Dataset: Standardized\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:60].astype(float)\r\n",
        "Y = dataset[:,60]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        "# baseline model\r\n",
        "def create_baseline():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\r\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "# evaluate baseline model with standardized dataset\r\n",
        "estimators = []\r\n",
        "estimators.append(('standardize', StandardScaler()))\r\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\r\n",
        "pipeline = Pipeline(estimators)\r\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\r\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: 84.62% (7.30%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdG3s9Pz-lTV",
        "outputId": "36b747ba-bc85-47bb-cc32-db58b247f720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# Regression Example With Boston Dataset: Baseline\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:13]\r\n",
        "Y = dataset[:,13]\r\n",
        "# define base model\r\n",
        "def baseline_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\r\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n",
        "\treturn model\r\n",
        "# evaluate model\r\n",
        "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\r\n",
        "kfold = KFold(n_splits=10)\r\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: -29.26 (23.29) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uu5lOf4_U-7",
        "outputId": "ba12532f-c882-4bb9-acf4-7dfb2d8fd4f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# Regression Example With Boston Dataset: Standardized\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:13]\r\n",
        "Y = dataset[:,13]\r\n",
        "# define base model\r\n",
        "def baseline_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\r\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n",
        "\treturn model\r\n",
        "# evaluate model with standardized dataset\r\n",
        "estimators = []\r\n",
        "estimators.append(('standardize', StandardScaler()))\r\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\r\n",
        "pipeline = Pipeline(estimators)\r\n",
        "kfold = KFold(n_splits=10)\r\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\r\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: -31.48 (30.93) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXFKvYIK_-Wa",
        "outputId": "86793a7e-f481-4d8d-efa4-1cf573d34117",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# Regression Example With Boston Dataset: Standardized and Larger\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:13]\r\n",
        "Y = dataset[:,13]\r\n",
        "# define the model\r\n",
        "def larger_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\r\n",
        "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\r\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n",
        "\treturn model\r\n",
        "# evaluate model with standardized dataset\r\n",
        "estimators = []\r\n",
        "estimators.append(('standardize', StandardScaler()))\r\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\r\n",
        "pipeline = Pipeline(estimators)\r\n",
        "kfold = KFold(n_splits=10)\r\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\r\n",
        "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger: -22.39 (24.13) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFpfdTOZ9rix",
        "outputId": "bbcd8dbb-a17d-434e-f644-55605c5f6e33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# Regression Example With Boston Dataset: Standardized and Wider\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:13]\r\n",
        "Y = dataset[:,13]\r\n",
        "# define wider model\r\n",
        "def wider_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\r\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\r\n",
        "\t# Compile model\r\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n",
        "\treturn model\r\n",
        "# evaluate model with standardized dataset\r\n",
        "estimators = []\r\n",
        "estimators.append(('standardize', StandardScaler()))\r\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\r\n",
        "pipeline = Pipeline(estimators)\r\n",
        "kfold = KFold(n_splits=10)\r\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\r\n",
        "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wider: -22.37 (26.53) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUpPyWnFASzB",
        "outputId": "18496528-8029-423e-88c5-be474f036fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import model_from_json\r\n",
        "import numpy\r\n",
        "import os\r\n",
        "# fix random seed for reproducibility\r\n",
        "numpy.random.seed(7)\r\n",
        "# load pima indians dataset\r\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\r\n",
        "# evaluate the model\r\n",
        "scores = model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\n",
        " \r\n",
        "# serialize model to JSON\r\n",
        "model_json = model.to_json()\r\n",
        "with open(\"model.json\", \"w\") as json_file:\r\n",
        "    json_file.write(model_json)\r\n",
        "# serialize weights to HDF5\r\n",
        "model.save_weights(\"model.h5\")\r\n",
        "print(\"Saved model to disk\")\r\n",
        " \r\n",
        "# later...\r\n",
        " \r\n",
        "# load json and create model\r\n",
        "json_file = open('model.json', 'r')\r\n",
        "loaded_model_json = json_file.read()\r\n",
        "json_file.close()\r\n",
        "loaded_model = model_from_json(loaded_model_json)\r\n",
        "# load weights into new model\r\n",
        "loaded_model.load_weights(\"model.h5\")\r\n",
        "print(\"Loaded model from disk\")\r\n",
        " \r\n",
        "# evaluate loaded model on test data\r\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 72.01%\n",
            "Saved model to disk\n",
            "Loaded model from disk\n",
            "accuracy: 72.01%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0SOhlvkBMRQ",
        "outputId": "9619864f-8e58-4ca9-8835-ee48aae566ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# MLP for Pima Indians Dataset serialize to YAML and HDF5\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import model_from_yaml\r\n",
        "import numpy\r\n",
        "import os\r\n",
        "# fix random seed for reproducibility\r\n",
        "seed = 7\r\n",
        "numpy.random.seed(seed)\r\n",
        "# load pima indians dataset\r\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\r\n",
        "# evaluate the model\r\n",
        "scores = model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\n",
        " \r\n",
        "# serialize model to YAML\r\n",
        "model_yaml = model.to_yaml()\r\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\r\n",
        "    yaml_file.write(model_yaml)\r\n",
        "# serialize weights to HDF5\r\n",
        "model.save_weights(\"model.h5\")\r\n",
        "print(\"Saved model to disk\")\r\n",
        " \r\n",
        "# later...\r\n",
        " \r\n",
        "# load YAML and create model\r\n",
        "yaml_file = open('model.yaml', 'r')\r\n",
        "loaded_model_yaml = yaml_file.read()\r\n",
        "yaml_file.close()\r\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\r\n",
        "# load weights into new model\r\n",
        "loaded_model.load_weights(\"model.h5\")\r\n",
        "print(\"Loaded model from disk\")\r\n",
        " \r\n",
        "# evaluate loaded model on test data\r\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 76.95%\n",
            "Saved model to disk\n",
            "Loaded model from disk\n",
            "accuracy: 76.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6lnZK2HB7mJ",
        "outputId": "4cbb7559-e00d-4ba5-9212-b2721cbc48f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# MLP for Pima Indians Dataset saved to single file\r\n",
        "from numpy import loadtxt\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "# load pima indians dataset\r\n",
        "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\r\n",
        "# evaluate the model\r\n",
        "scores = model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\n",
        "# save model and architecture to single file\r\n",
        "model.save(\"model.h5\")\r\n",
        "print(\"Saved model to disk\")\r\n",
        "\r\n",
        "\r\n",
        "# load and evaluate a saved model\r\n",
        "from numpy import loadtxt\r\n",
        "from keras.models import load_model\r\n",
        "\r\n",
        "# load model\r\n",
        "model = load_model('model.h5')\r\n",
        "# summarize model.\r\n",
        "model.summary()\r\n",
        "# load dataset\r\n",
        "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# evaluate the model\r\n",
        "score = model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 76.82%\n",
            "Saved model to disk\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_38 (Dense)             (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 221\n",
            "Trainable params: 221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "accuracy: 76.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UneDY0NLybj"
      },
      "source": [
        "# Checkpoint the weights when validation accuracy improves\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy\r\n",
        "numpy.random.seed(seed)\r\n",
        "# load pima indians dataset\r\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# checkpoint\r\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEeBRJqpMmjk"
      },
      "source": [
        "# Checkpoint the weights for best model on validation accuracy\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy\r\n",
        "# load pima indians dataset\r\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# checkpoint\r\n",
        "filepath=\"weights.best.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFoGQa-MNPnS",
        "outputId": "f15726bf-7b32-483c-f3ce-283e92478dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# How to load and use weights from a checkpoint\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# load weights\r\n",
        "model.load_weights(\"weights.best.hdf5\")\r\n",
        "# Compile model (required to make predictions)\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "print(\"Created model and loaded weights from file\")\r\n",
        "# load pima indians dataset\r\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# estimate accuracy on whole dataset using loaded weights\r\n",
        "scores = model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created model and loaded weights from file\n",
            "accuracy: 75.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDAwpwJP8Al"
      },
      "source": [
        "# Visualize training history\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy\r\n",
        "# load pima indians dataset\r\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "Y = dataset[:,8]\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# Fit the model\r\n",
        "history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)\r\n",
        "# list all data in history\r\n",
        "print(history.history.keys())\r\n",
        "# summarize history for accuracy\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "# summarize history for loss\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhF6x3npSqaq",
        "outputId": "11195cc8-52ae-4056-ae29-9a5f05316921",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Baseline Model on the Sonar Dataset\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.optimizers import SGD\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:60].astype(float)\r\n",
        "Y = dataset[:,60]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        " \r\n",
        "# baseline\r\n",
        "def create_baseline():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\r\n",
        "\tmodel.add(Dense(30,  activation='relu'))\r\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\r\n",
        "\t# Compile model\r\n",
        "\tsgd = SGD(lr=0.01, momentum=0.8)\r\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        " \r\n",
        "estimators = []\r\n",
        "estimators.append(('standardize', StandardScaler()))\r\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\r\n",
        "pipeline = Pipeline(estimators)\r\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\r\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 3907 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454d3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 3909 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa48462d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454e59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a53ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a3cbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a2e6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa453c9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 86.48% (8.44%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYj0izr6S-Dk",
        "outputId": "2d91d9bc-7671-4dc5-c377-895b318acf0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example of Dropout on the Sonar Dataset: Visible Layer\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.constraints import maxnorm\r\n",
        "from keras.optimizers import SGD\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:60].astype(float)\r\n",
        "Y = dataset[:,60]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        " \r\n",
        "# dropout in the input layer with weight constraint\r\n",
        "def create_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dropout(0.2, input_shape=(60,)))\r\n",
        "\tmodel.add(Dense(60, activation='relu', kernel_constraint=maxnorm(3)))\r\n",
        "\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\r\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\r\n",
        "\t# Compile model\r\n",
        "\tsgd = SGD(lr=0.1, momentum=0.9)\r\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        " \r\n",
        "estimators = []\r\n",
        "estimators.append(('standardize', StandardScaler()))\r\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\r\n",
        "pipeline = Pipeline(estimators)\r\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\r\n",
        "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451f9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa455d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa453c91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454e06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451a1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451a1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45595ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a45d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45472488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45472c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Visible: 90.40% (6.45%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP6i_g1ZTXXN",
        "outputId": "ffde770b-2237-443f-f3a0-7abc87a468da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example of Dropout on the Sonar Dataset: Hidden Layer\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.constraints import maxnorm\r\n",
        "from keras.optimizers import SGD\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:60].astype(float)\r\n",
        "Y = dataset[:,60]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "encoded_Y = encoder.transform(Y)\r\n",
        " \r\n",
        "# dropout in hidden layers with weight constraint\r\n",
        "def create_model():\r\n",
        "\t# create model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=maxnorm(3)))\r\n",
        "\tmodel.add(Dropout(0.2))\r\n",
        "\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\r\n",
        "\tmodel.add(Dropout(0.2))\r\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\r\n",
        "\t# Compile model\r\n",
        "\tsgd = SGD(lr=0.1, momentum=0.9)\r\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        " \r\n",
        "estimators = []\r\n",
        "estimators.append(('standardize', StandardScaler()))\r\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\r\n",
        "pipeline = Pipeline(estimators)\r\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\r\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\r\n",
        "print(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4bc4b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4b3660d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451deae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451de0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451de048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454e5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa452991e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45595048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa3ae1a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451a1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Hidden: 81.69% (7.73%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG76_cBLV0GL"
      },
      "source": [
        "# Time Based Learning Rate Decay\r\n",
        "from pandas import read_csv\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.optimizers import SGD\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"ionosphere.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:34].astype(float)\r\n",
        "Y = dataset[:,34]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "Y = encoder.transform(Y)\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(34, input_dim=34, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "epochs = 50\r\n",
        "learning_rate = 0.1\r\n",
        "decay_rate = learning_rate / epochs\r\n",
        "momentum = 0.8\r\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\r\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN_bKSqDXpF1",
        "outputId": "18babe37-2e1e-4b17-a1ea-63d36b6b686e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Drop-Based Learning Rate Decay\r\n",
        "from pandas import read_csv\r\n",
        "import math\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.optimizers import SGD\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.callbacks import LearningRateScheduler\r\n",
        " \r\n",
        "# learning rate schedule\r\n",
        "def step_decay(epoch):\r\n",
        "\tinitial_lrate = 0.1\r\n",
        "\tdrop = 0.5\r\n",
        "\tepochs_drop = 10.0\r\n",
        "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\r\n",
        "\treturn lrate\r\n",
        " \r\n",
        "# load dataset\r\n",
        "dataframe = read_csv(\"ionosphere.csv\", header=None)\r\n",
        "dataset = dataframe.values\r\n",
        "# split into input (X) and output (Y) variables\r\n",
        "X = dataset[:,0:34].astype(float)\r\n",
        "Y = dataset[:,34]\r\n",
        "# encode class values as integers\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(Y)\r\n",
        "Y = encoder.transform(Y)\r\n",
        "# create model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(34, input_dim=34, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# Compile model\r\n",
        "sgd = SGD(lr=0.0, momentum=0.9)\r\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n",
        "# learning schedule callback\r\n",
        "lrate = LearningRateScheduler(step_decay)\r\n",
        "callbacks_list = [lrate]\r\n",
        "# Fit the model\r\n",
        "model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9/9 - 1s - loss: 0.6434 - accuracy: 0.5702 - val_loss: 0.4615 - val_accuracy: 0.8621\n",
            "Epoch 2/50\n",
            "9/9 - 0s - loss: 0.3983 - accuracy: 0.8553 - val_loss: 0.1839 - val_accuracy: 0.9655\n",
            "Epoch 3/50\n",
            "9/9 - 0s - loss: 0.3332 - accuracy: 0.8723 - val_loss: 0.3178 - val_accuracy: 0.9310\n",
            "Epoch 4/50\n",
            "9/9 - 0s - loss: 0.2141 - accuracy: 0.9191 - val_loss: 0.1151 - val_accuracy: 0.9741\n",
            "Epoch 5/50\n",
            "9/9 - 0s - loss: 0.1660 - accuracy: 0.9447 - val_loss: 0.1624 - val_accuracy: 0.9741\n",
            "Epoch 6/50\n",
            "9/9 - 0s - loss: 0.1484 - accuracy: 0.9574 - val_loss: 0.1003 - val_accuracy: 0.9828\n",
            "Epoch 7/50\n",
            "9/9 - 0s - loss: 0.1280 - accuracy: 0.9702 - val_loss: 0.1223 - val_accuracy: 0.9828\n",
            "Epoch 8/50\n",
            "9/9 - 0s - loss: 0.1116 - accuracy: 0.9660 - val_loss: 0.0978 - val_accuracy: 0.9914\n",
            "Epoch 9/50\n",
            "9/9 - 0s - loss: 0.1006 - accuracy: 0.9745 - val_loss: 0.0631 - val_accuracy: 0.9914\n",
            "Epoch 10/50\n",
            "9/9 - 0s - loss: 0.0867 - accuracy: 0.9830 - val_loss: 0.0843 - val_accuracy: 0.9914\n",
            "Epoch 11/50\n",
            "9/9 - 0s - loss: 0.0815 - accuracy: 0.9830 - val_loss: 0.0680 - val_accuracy: 0.9914\n",
            "Epoch 12/50\n",
            "9/9 - 0s - loss: 0.0766 - accuracy: 0.9830 - val_loss: 0.0741 - val_accuracy: 0.9914\n",
            "Epoch 13/50\n",
            "9/9 - 0s - loss: 0.0730 - accuracy: 0.9830 - val_loss: 0.0687 - val_accuracy: 0.9914\n",
            "Epoch 14/50\n",
            "9/9 - 0s - loss: 0.0692 - accuracy: 0.9830 - val_loss: 0.0697 - val_accuracy: 0.9914\n",
            "Epoch 15/50\n",
            "9/9 - 0s - loss: 0.0658 - accuracy: 0.9830 - val_loss: 0.0712 - val_accuracy: 0.9914\n",
            "Epoch 16/50\n",
            "9/9 - 0s - loss: 0.0663 - accuracy: 0.9830 - val_loss: 0.0625 - val_accuracy: 0.9914\n",
            "Epoch 17/50\n",
            "9/9 - 0s - loss: 0.0627 - accuracy: 0.9830 - val_loss: 0.0710 - val_accuracy: 0.9914\n",
            "Epoch 18/50\n",
            "9/9 - 0s - loss: 0.0644 - accuracy: 0.9830 - val_loss: 0.0567 - val_accuracy: 0.9914\n",
            "Epoch 19/50\n",
            "9/9 - 0s - loss: 0.0645 - accuracy: 0.9830 - val_loss: 0.0550 - val_accuracy: 0.9914\n",
            "Epoch 20/50\n",
            "9/9 - 0s - loss: 0.0682 - accuracy: 0.9830 - val_loss: 0.1024 - val_accuracy: 0.9828\n",
            "Epoch 21/50\n",
            "9/9 - 0s - loss: 0.0605 - accuracy: 0.9830 - val_loss: 0.0472 - val_accuracy: 0.9914\n",
            "Epoch 22/50\n",
            "9/9 - 0s - loss: 0.0563 - accuracy: 0.9830 - val_loss: 0.0567 - val_accuracy: 0.9914\n",
            "Epoch 23/50\n",
            "9/9 - 0s - loss: 0.0564 - accuracy: 0.9830 - val_loss: 0.0710 - val_accuracy: 0.9914\n",
            "Epoch 24/50\n",
            "9/9 - 0s - loss: 0.0510 - accuracy: 0.9830 - val_loss: 0.0493 - val_accuracy: 0.9914\n",
            "Epoch 25/50\n",
            "9/9 - 0s - loss: 0.0529 - accuracy: 0.9830 - val_loss: 0.0513 - val_accuracy: 0.9914\n",
            "Epoch 26/50\n",
            "9/9 - 0s - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.0644 - val_accuracy: 0.9914\n",
            "Epoch 27/50\n",
            "9/9 - 0s - loss: 0.0486 - accuracy: 0.9830 - val_loss: 0.0588 - val_accuracy: 0.9914\n",
            "Epoch 28/50\n",
            "9/9 - 0s - loss: 0.0475 - accuracy: 0.9830 - val_loss: 0.0534 - val_accuracy: 0.9914\n",
            "Epoch 29/50\n",
            "9/9 - 0s - loss: 0.0474 - accuracy: 0.9830 - val_loss: 0.0569 - val_accuracy: 0.9914\n",
            "Epoch 30/50\n",
            "9/9 - 0s - loss: 0.0472 - accuracy: 0.9830 - val_loss: 0.0489 - val_accuracy: 0.9914\n",
            "Epoch 31/50\n",
            "9/9 - 0s - loss: 0.0460 - accuracy: 0.9830 - val_loss: 0.0509 - val_accuracy: 0.9914\n",
            "Epoch 32/50\n",
            "9/9 - 0s - loss: 0.0454 - accuracy: 0.9872 - val_loss: 0.0555 - val_accuracy: 0.9914\n",
            "Epoch 33/50\n",
            "9/9 - 0s - loss: 0.0453 - accuracy: 0.9872 - val_loss: 0.0538 - val_accuracy: 0.9914\n",
            "Epoch 34/50\n",
            "9/9 - 0s - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0549 - val_accuracy: 0.9914\n",
            "Epoch 35/50\n",
            "9/9 - 0s - loss: 0.0442 - accuracy: 0.9872 - val_loss: 0.0536 - val_accuracy: 0.9914\n",
            "Epoch 36/50\n",
            "9/9 - 0s - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0561 - val_accuracy: 0.9914\n",
            "Epoch 37/50\n",
            "9/9 - 0s - loss: 0.0436 - accuracy: 0.9872 - val_loss: 0.0530 - val_accuracy: 0.9914\n",
            "Epoch 38/50\n",
            "9/9 - 0s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0532 - val_accuracy: 0.9914\n",
            "Epoch 39/50\n",
            "9/9 - 0s - loss: 0.0430 - accuracy: 0.9872 - val_loss: 0.0536 - val_accuracy: 0.9914\n",
            "Epoch 40/50\n",
            "9/9 - 0s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0515 - val_accuracy: 0.9914\n",
            "Epoch 41/50\n",
            "9/9 - 0s - loss: 0.0435 - accuracy: 0.9872 - val_loss: 0.0592 - val_accuracy: 0.9914\n",
            "Epoch 42/50\n",
            "9/9 - 0s - loss: 0.0426 - accuracy: 0.9915 - val_loss: 0.0575 - val_accuracy: 0.9914\n",
            "Epoch 43/50\n",
            "9/9 - 0s - loss: 0.0423 - accuracy: 0.9915 - val_loss: 0.0566 - val_accuracy: 0.9914\n",
            "Epoch 44/50\n",
            "9/9 - 0s - loss: 0.0418 - accuracy: 0.9915 - val_loss: 0.0538 - val_accuracy: 0.9914\n",
            "Epoch 45/50\n",
            "9/9 - 0s - loss: 0.0418 - accuracy: 0.9872 - val_loss: 0.0523 - val_accuracy: 0.9914\n",
            "Epoch 46/50\n",
            "9/9 - 0s - loss: 0.0417 - accuracy: 0.9872 - val_loss: 0.0526 - val_accuracy: 0.9914\n",
            "Epoch 47/50\n",
            "9/9 - 0s - loss: 0.0415 - accuracy: 0.9872 - val_loss: 0.0521 - val_accuracy: 0.9914\n",
            "Epoch 48/50\n",
            "9/9 - 0s - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0517 - val_accuracy: 0.9914\n",
            "Epoch 49/50\n",
            "9/9 - 0s - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0531 - val_accuracy: 0.9914\n",
            "Epoch 50/50\n",
            "9/9 - 0s - loss: 0.0411 - accuracy: 0.9872 - val_loss: 0.0526 - val_accuracy: 0.9914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faa4a447518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}