{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JBLeeDeepLearn_First_NeuralNet_WithKeras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athiagarajan/JBLeeFiles/blob/master/JBLeeDeepLearn_First_NeuralNet_WithKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtmMhqfsL9Bw"
      },
      "source": [
        "# **First Neural Network With Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74XwKafVMDeU",
        "outputId": "56935c36-09fc-45fd-f363-49e02d83a1ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create your first MLP in Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "# Compile model\n",
        "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "# Fit the model (verbose=0 for no output and verbose=1 for output)\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step - loss: 0.4832 - accuracy: 0.7643\n",
            "accuracy: 76.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOht0CaHPl3U"
      },
      "source": [
        "# **Evaluating Performance of Deep Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o65_qHYVPtXG",
        "outputId": "bd0e2cfc-4061-45a7-e405-70005290ade6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create your first MLP in Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "# Compile model\n",
        "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "52/52 [==============================] - 1s 4ms/step - loss: 0.6906 - accuracy: 0.5791 - val_loss: 0.6760 - val_accuracy: 0.6732\n",
            "Epoch 2/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6789 - accuracy: 0.6435 - val_loss: 0.6643 - val_accuracy: 0.6732\n",
            "Epoch 3/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6770 - accuracy: 0.6229 - val_loss: 0.6565 - val_accuracy: 0.6732\n",
            "Epoch 4/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6745 - accuracy: 0.6156 - val_loss: 0.6500 - val_accuracy: 0.6732\n",
            "Epoch 5/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.5919 - val_loss: 0.6446 - val_accuracy: 0.6732\n",
            "Epoch 6/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.6443 - val_loss: 0.6384 - val_accuracy: 0.6732\n",
            "Epoch 7/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6308 - accuracy: 0.6571 - val_loss: 0.6332 - val_accuracy: 0.6732\n",
            "Epoch 8/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.5988 - val_loss: 0.6268 - val_accuracy: 0.6732\n",
            "Epoch 9/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.6457 - val_loss: 0.6212 - val_accuracy: 0.6732\n",
            "Epoch 10/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6284 - val_loss: 0.6161 - val_accuracy: 0.6732\n",
            "Epoch 11/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.6514 - val_loss: 0.6244 - val_accuracy: 0.6614\n",
            "Epoch 12/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6068 - accuracy: 0.6793 - val_loss: 0.6089 - val_accuracy: 0.6378\n",
            "Epoch 13/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.7320 - val_loss: 0.6067 - val_accuracy: 0.6378\n",
            "Epoch 14/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5902 - accuracy: 0.6867 - val_loss: 0.6093 - val_accuracy: 0.6417\n",
            "Epoch 15/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6325 - accuracy: 0.6521 - val_loss: 0.6011 - val_accuracy: 0.6339\n",
            "Epoch 16/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.6737 - val_loss: 0.5987 - val_accuracy: 0.6339\n",
            "Epoch 17/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.6869 - val_loss: 0.5967 - val_accuracy: 0.6457\n",
            "Epoch 18/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.6705 - val_loss: 0.5945 - val_accuracy: 0.6339\n",
            "Epoch 19/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5904 - accuracy: 0.6741 - val_loss: 0.5921 - val_accuracy: 0.6417\n",
            "Epoch 20/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.6742 - val_loss: 0.5972 - val_accuracy: 0.6417\n",
            "Epoch 21/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5750 - accuracy: 0.7190 - val_loss: 0.5912 - val_accuracy: 0.6417\n",
            "Epoch 22/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6009 - accuracy: 0.6851 - val_loss: 0.5905 - val_accuracy: 0.6417\n",
            "Epoch 23/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.6797 - val_loss: 0.5942 - val_accuracy: 0.6417\n",
            "Epoch 24/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.7157 - val_loss: 0.5892 - val_accuracy: 0.6417\n",
            "Epoch 25/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6068 - accuracy: 0.6459 - val_loss: 0.5856 - val_accuracy: 0.6496\n",
            "Epoch 26/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5726 - accuracy: 0.7071 - val_loss: 0.5985 - val_accuracy: 0.6378\n",
            "Epoch 27/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5832 - accuracy: 0.6999 - val_loss: 0.5848 - val_accuracy: 0.6535\n",
            "Epoch 28/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7409 - val_loss: 0.5840 - val_accuracy: 0.6457\n",
            "Epoch 29/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.6869 - val_loss: 0.5832 - val_accuracy: 0.6496\n",
            "Epoch 30/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5774 - accuracy: 0.7149 - val_loss: 0.5848 - val_accuracy: 0.6535\n",
            "Epoch 31/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.6922 - val_loss: 0.5884 - val_accuracy: 0.6417\n",
            "Epoch 32/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5725 - accuracy: 0.6820 - val_loss: 0.5822 - val_accuracy: 0.6496\n",
            "Epoch 33/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.6800 - val_loss: 0.5836 - val_accuracy: 0.6614\n",
            "Epoch 34/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5969 - accuracy: 0.6922 - val_loss: 0.5900 - val_accuracy: 0.6417\n",
            "Epoch 35/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.7064 - val_loss: 0.5935 - val_accuracy: 0.6417\n",
            "Epoch 36/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.6935 - val_loss: 0.5861 - val_accuracy: 0.6457\n",
            "Epoch 37/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.7149 - val_loss: 0.5803 - val_accuracy: 0.6496\n",
            "Epoch 38/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.6884 - val_loss: 0.5851 - val_accuracy: 0.6535\n",
            "Epoch 39/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.7021 - val_loss: 0.5810 - val_accuracy: 0.6614\n",
            "Epoch 40/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7102 - val_loss: 0.5816 - val_accuracy: 0.6535\n",
            "Epoch 41/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.6997 - val_loss: 0.5826 - val_accuracy: 0.6575\n",
            "Epoch 42/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.6998 - val_loss: 0.5770 - val_accuracy: 0.6535\n",
            "Epoch 43/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.6973 - val_loss: 0.5876 - val_accuracy: 0.6496\n",
            "Epoch 44/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.6816 - val_loss: 0.5779 - val_accuracy: 0.6575\n",
            "Epoch 45/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.7442 - val_loss: 0.5795 - val_accuracy: 0.6535\n",
            "Epoch 46/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5856 - accuracy: 0.6952 - val_loss: 0.5776 - val_accuracy: 0.6575\n",
            "Epoch 47/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7288 - val_loss: 0.5756 - val_accuracy: 0.6575\n",
            "Epoch 48/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7055 - val_loss: 0.5829 - val_accuracy: 0.6496\n",
            "Epoch 49/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7165 - val_loss: 0.5751 - val_accuracy: 0.6614\n",
            "Epoch 50/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5671 - accuracy: 0.7241 - val_loss: 0.5755 - val_accuracy: 0.6575\n",
            "Epoch 51/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7287 - val_loss: 0.5743 - val_accuracy: 0.6614\n",
            "Epoch 52/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.6814 - val_loss: 0.5733 - val_accuracy: 0.6575\n",
            "Epoch 53/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.6907 - val_loss: 0.5758 - val_accuracy: 0.6457\n",
            "Epoch 54/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.7042 - val_loss: 0.5750 - val_accuracy: 0.6654\n",
            "Epoch 55/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5781 - accuracy: 0.6923 - val_loss: 0.5751 - val_accuracy: 0.6496\n",
            "Epoch 56/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5378 - accuracy: 0.7287 - val_loss: 0.5799 - val_accuracy: 0.6614\n",
            "Epoch 57/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.7022 - val_loss: 0.5721 - val_accuracy: 0.6496\n",
            "Epoch 58/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.7059 - val_loss: 0.5776 - val_accuracy: 0.6575\n",
            "Epoch 59/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.7050 - val_loss: 0.5787 - val_accuracy: 0.6614\n",
            "Epoch 60/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7383 - val_loss: 0.5716 - val_accuracy: 0.6535\n",
            "Epoch 61/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.6798 - val_loss: 0.5862 - val_accuracy: 0.6496\n",
            "Epoch 62/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7103 - val_loss: 0.5714 - val_accuracy: 0.6496\n",
            "Epoch 63/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.6683 - val_loss: 0.5801 - val_accuracy: 0.6614\n",
            "Epoch 64/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.7041 - val_loss: 0.5711 - val_accuracy: 0.6614\n",
            "Epoch 65/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5412 - accuracy: 0.7420 - val_loss: 0.5712 - val_accuracy: 0.6575\n",
            "Epoch 66/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.7010 - val_loss: 0.5722 - val_accuracy: 0.6614\n",
            "Epoch 67/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.6943 - val_loss: 0.5743 - val_accuracy: 0.6575\n",
            "Epoch 68/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.6967 - val_loss: 0.5700 - val_accuracy: 0.6693\n",
            "Epoch 69/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.7151 - val_loss: 0.5693 - val_accuracy: 0.6535\n",
            "Epoch 70/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7270 - val_loss: 0.5804 - val_accuracy: 0.6811\n",
            "Epoch 71/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7301 - val_loss: 0.5670 - val_accuracy: 0.6614\n",
            "Epoch 72/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7321 - val_loss: 0.5677 - val_accuracy: 0.6496\n",
            "Epoch 73/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7071 - val_loss: 0.5692 - val_accuracy: 0.6614\n",
            "Epoch 74/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5407 - accuracy: 0.7421 - val_loss: 0.5690 - val_accuracy: 0.6535\n",
            "Epoch 75/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7263 - val_loss: 0.5694 - val_accuracy: 0.6654\n",
            "Epoch 76/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5597 - accuracy: 0.7097 - val_loss: 0.5667 - val_accuracy: 0.6614\n",
            "Epoch 77/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5602 - accuracy: 0.7179 - val_loss: 0.5680 - val_accuracy: 0.6693\n",
            "Epoch 78/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7084 - val_loss: 0.5731 - val_accuracy: 0.6654\n",
            "Epoch 79/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5467 - accuracy: 0.7074 - val_loss: 0.5664 - val_accuracy: 0.6614\n",
            "Epoch 80/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7016 - val_loss: 0.5659 - val_accuracy: 0.6614\n",
            "Epoch 81/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5676 - accuracy: 0.7233 - val_loss: 0.5629 - val_accuracy: 0.6693\n",
            "Epoch 82/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.7045 - val_loss: 0.5659 - val_accuracy: 0.6575\n",
            "Epoch 83/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7224 - val_loss: 0.5635 - val_accuracy: 0.6732\n",
            "Epoch 84/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.7014 - val_loss: 0.5640 - val_accuracy: 0.6614\n",
            "Epoch 85/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5439 - accuracy: 0.7097 - val_loss: 0.5607 - val_accuracy: 0.6614\n",
            "Epoch 86/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5532 - accuracy: 0.7245 - val_loss: 0.5611 - val_accuracy: 0.6535\n",
            "Epoch 87/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5680 - accuracy: 0.7076 - val_loss: 0.5621 - val_accuracy: 0.6772\n",
            "Epoch 88/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7429 - val_loss: 0.5622 - val_accuracy: 0.6693\n",
            "Epoch 89/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7679 - val_loss: 0.5748 - val_accuracy: 0.6969\n",
            "Epoch 90/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7583 - val_loss: 0.5591 - val_accuracy: 0.6772\n",
            "Epoch 91/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7106 - val_loss: 0.5594 - val_accuracy: 0.6772\n",
            "Epoch 92/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5602 - accuracy: 0.6969 - val_loss: 0.5597 - val_accuracy: 0.6890\n",
            "Epoch 93/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5267 - accuracy: 0.7496 - val_loss: 0.5636 - val_accuracy: 0.6772\n",
            "Epoch 94/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.7298 - val_loss: 0.5624 - val_accuracy: 0.6772\n",
            "Epoch 95/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5506 - accuracy: 0.7160 - val_loss: 0.5651 - val_accuracy: 0.6850\n",
            "Epoch 96/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.6911 - val_loss: 0.5685 - val_accuracy: 0.6850\n",
            "Epoch 97/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7140 - val_loss: 0.5617 - val_accuracy: 0.6772\n",
            "Epoch 98/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7655 - val_loss: 0.5558 - val_accuracy: 0.7087\n",
            "Epoch 99/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5388 - accuracy: 0.7249 - val_loss: 0.5594 - val_accuracy: 0.6929\n",
            "Epoch 100/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.6733 - val_loss: 0.5671 - val_accuracy: 0.6850\n",
            "Epoch 101/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7290 - val_loss: 0.5607 - val_accuracy: 0.6890\n",
            "Epoch 102/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5453 - accuracy: 0.7240 - val_loss: 0.5677 - val_accuracy: 0.6929\n",
            "Epoch 103/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7408 - val_loss: 0.5685 - val_accuracy: 0.6772\n",
            "Epoch 104/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5518 - accuracy: 0.7459 - val_loss: 0.5543 - val_accuracy: 0.7008\n",
            "Epoch 105/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5435 - accuracy: 0.7198 - val_loss: 0.5596 - val_accuracy: 0.6929\n",
            "Epoch 106/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7114 - val_loss: 0.5615 - val_accuracy: 0.6850\n",
            "Epoch 107/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7244 - val_loss: 0.5609 - val_accuracy: 0.6850\n",
            "Epoch 108/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7384 - val_loss: 0.5571 - val_accuracy: 0.6929\n",
            "Epoch 109/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7638 - val_loss: 0.5606 - val_accuracy: 0.7008\n",
            "Epoch 110/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7402 - val_loss: 0.5608 - val_accuracy: 0.6732\n",
            "Epoch 111/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7207 - val_loss: 0.5540 - val_accuracy: 0.7008\n",
            "Epoch 112/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7599 - val_loss: 0.5528 - val_accuracy: 0.6969\n",
            "Epoch 113/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7541 - val_loss: 0.5536 - val_accuracy: 0.6929\n",
            "Epoch 114/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7440 - val_loss: 0.5499 - val_accuracy: 0.7087\n",
            "Epoch 115/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7547 - val_loss: 0.5498 - val_accuracy: 0.7126\n",
            "Epoch 116/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7237 - val_loss: 0.5527 - val_accuracy: 0.6929\n",
            "Epoch 117/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.7369 - val_loss: 0.5539 - val_accuracy: 0.7047\n",
            "Epoch 118/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7409 - val_loss: 0.5562 - val_accuracy: 0.6850\n",
            "Epoch 119/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5275 - accuracy: 0.7484 - val_loss: 0.5515 - val_accuracy: 0.6969\n",
            "Epoch 120/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7355 - val_loss: 0.5483 - val_accuracy: 0.7126\n",
            "Epoch 121/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7537 - val_loss: 0.5421 - val_accuracy: 0.7205\n",
            "Epoch 122/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7703 - val_loss: 0.5543 - val_accuracy: 0.7205\n",
            "Epoch 123/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7178 - val_loss: 0.5481 - val_accuracy: 0.7087\n",
            "Epoch 124/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7583 - val_loss: 0.5536 - val_accuracy: 0.6929\n",
            "Epoch 125/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7536 - val_loss: 0.5406 - val_accuracy: 0.7205\n",
            "Epoch 126/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7664 - val_loss: 0.5432 - val_accuracy: 0.7047\n",
            "Epoch 127/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7433 - val_loss: 0.5398 - val_accuracy: 0.7126\n",
            "Epoch 128/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7460 - val_loss: 0.5479 - val_accuracy: 0.7047\n",
            "Epoch 129/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7409 - val_loss: 0.5520 - val_accuracy: 0.7087\n",
            "Epoch 130/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7294 - val_loss: 0.5457 - val_accuracy: 0.7087\n",
            "Epoch 131/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7508 - val_loss: 0.5381 - val_accuracy: 0.7165\n",
            "Epoch 132/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7430 - val_loss: 0.5476 - val_accuracy: 0.7165\n",
            "Epoch 133/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.7160 - val_loss: 0.5456 - val_accuracy: 0.7087\n",
            "Epoch 134/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7651 - val_loss: 0.5664 - val_accuracy: 0.7047\n",
            "Epoch 135/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5077 - accuracy: 0.7736 - val_loss: 0.5347 - val_accuracy: 0.7205\n",
            "Epoch 136/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7408 - val_loss: 0.5356 - val_accuracy: 0.7244\n",
            "Epoch 137/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7343 - val_loss: 0.5404 - val_accuracy: 0.7205\n",
            "Epoch 138/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7717 - val_loss: 0.5394 - val_accuracy: 0.7165\n",
            "Epoch 139/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7631 - val_loss: 0.5435 - val_accuracy: 0.7087\n",
            "Epoch 140/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7267 - val_loss: 0.5367 - val_accuracy: 0.7205\n",
            "Epoch 141/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7592 - val_loss: 0.5348 - val_accuracy: 0.7323\n",
            "Epoch 142/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7570 - val_loss: 0.5424 - val_accuracy: 0.7047\n",
            "Epoch 143/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7464 - val_loss: 0.5371 - val_accuracy: 0.7087\n",
            "Epoch 144/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7524 - val_loss: 0.5354 - val_accuracy: 0.7087\n",
            "Epoch 145/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7568 - val_loss: 0.5429 - val_accuracy: 0.7165\n",
            "Epoch 146/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7945 - val_loss: 0.5423 - val_accuracy: 0.7244\n",
            "Epoch 147/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7269 - val_loss: 0.5475 - val_accuracy: 0.7087\n",
            "Epoch 148/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.7129 - val_loss: 0.5497 - val_accuracy: 0.7244\n",
            "Epoch 149/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7501 - val_loss: 0.5387 - val_accuracy: 0.7047\n",
            "Epoch 150/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.7437 - val_loss: 0.5404 - val_accuracy: 0.7205\n",
            "24/24 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7461\n",
            "accuracy: 74.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQiK9OAQR-9",
        "outputId": "bb83f872-a7a9-482b-cad5-8061fa2955b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MLP with manual validation set\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# split into 67% for train and 33% for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "# Compile model\n",
        "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.6901 - accuracy: 0.6357 WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4bb210440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.6898 - accuracy: 0.6367 - val_loss: 0.6785 - val_accuracy: 0.6378\n",
            "Epoch 2/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.6736 - val_loss: 0.6704 - val_accuracy: 0.6378\n",
            "Epoch 3/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.6639 - val_loss: 0.6651 - val_accuracy: 0.6378\n",
            "Epoch 4/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6494 - accuracy: 0.6686 - val_loss: 0.6553 - val_accuracy: 0.6378\n",
            "Epoch 5/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6273 - accuracy: 0.6939 - val_loss: 0.6575 - val_accuracy: 0.6378\n",
            "Epoch 6/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.6662 - val_loss: 0.6420 - val_accuracy: 0.6378\n",
            "Epoch 7/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6358 - val_loss: 0.6329 - val_accuracy: 0.6378\n",
            "Epoch 8/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.6603 - val_loss: 0.6242 - val_accuracy: 0.6378\n",
            "Epoch 9/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6267 - accuracy: 0.6447 - val_loss: 0.6178 - val_accuracy: 0.6378\n",
            "Epoch 10/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.6355 - val_loss: 0.6181 - val_accuracy: 0.6378\n",
            "Epoch 11/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.6539 - val_loss: 0.6110 - val_accuracy: 0.6378\n",
            "Epoch 12/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.6243 - val_loss: 0.6188 - val_accuracy: 0.6378\n",
            "Epoch 13/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.6790 - val_loss: 0.6109 - val_accuracy: 0.6378\n",
            "Epoch 14/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6000 - accuracy: 0.6493 - val_loss: 0.6002 - val_accuracy: 0.6969\n",
            "Epoch 15/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5719 - accuracy: 0.7469 - val_loss: 0.5935 - val_accuracy: 0.6969\n",
            "Epoch 16/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6089 - accuracy: 0.6737 - val_loss: 0.5907 - val_accuracy: 0.7008\n",
            "Epoch 17/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5781 - accuracy: 0.7143 - val_loss: 0.6006 - val_accuracy: 0.6693\n",
            "Epoch 18/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.6699 - val_loss: 0.5870 - val_accuracy: 0.6890\n",
            "Epoch 19/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.7054 - val_loss: 0.5869 - val_accuracy: 0.6890\n",
            "Epoch 20/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7109 - val_loss: 0.5848 - val_accuracy: 0.7047\n",
            "Epoch 21/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.7087 - val_loss: 0.5867 - val_accuracy: 0.6890\n",
            "Epoch 22/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.6926 - val_loss: 0.5808 - val_accuracy: 0.6969\n",
            "Epoch 23/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5935 - accuracy: 0.6696 - val_loss: 0.5821 - val_accuracy: 0.7008\n",
            "Epoch 24/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7139 - val_loss: 0.5807 - val_accuracy: 0.6969\n",
            "Epoch 25/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5720 - accuracy: 0.7020 - val_loss: 0.5802 - val_accuracy: 0.6969\n",
            "Epoch 26/150\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.5608 - accuracy: 0.7234 - val_loss: 0.5827 - val_accuracy: 0.6969\n",
            "Epoch 27/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.6911 - val_loss: 0.5865 - val_accuracy: 0.6969\n",
            "Epoch 28/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7361 - val_loss: 0.5834 - val_accuracy: 0.7008\n",
            "Epoch 29/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7195 - val_loss: 0.5929 - val_accuracy: 0.6929\n",
            "Epoch 30/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7050 - val_loss: 0.5750 - val_accuracy: 0.6890\n",
            "Epoch 31/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7313 - val_loss: 0.5760 - val_accuracy: 0.7008\n",
            "Epoch 32/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.6753 - val_loss: 0.5788 - val_accuracy: 0.6929\n",
            "Epoch 33/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.7106 - val_loss: 0.5744 - val_accuracy: 0.6969\n",
            "Epoch 34/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.7103 - val_loss: 0.5725 - val_accuracy: 0.6929\n",
            "Epoch 35/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.7113 - val_loss: 0.5704 - val_accuracy: 0.6929\n",
            "Epoch 36/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.7051 - val_loss: 0.5704 - val_accuracy: 0.7047\n",
            "Epoch 37/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.6942 - val_loss: 0.5830 - val_accuracy: 0.6890\n",
            "Epoch 38/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.6710 - val_loss: 0.5669 - val_accuracy: 0.7087\n",
            "Epoch 39/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.7069 - val_loss: 0.5749 - val_accuracy: 0.7047\n",
            "Epoch 40/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6012 - accuracy: 0.6717 - val_loss: 0.5654 - val_accuracy: 0.6969\n",
            "Epoch 41/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.6808 - val_loss: 0.5640 - val_accuracy: 0.6890\n",
            "Epoch 42/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.6843 - val_loss: 0.5740 - val_accuracy: 0.7008\n",
            "Epoch 43/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7262 - val_loss: 0.5652 - val_accuracy: 0.7126\n",
            "Epoch 44/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7286 - val_loss: 0.5643 - val_accuracy: 0.7047\n",
            "Epoch 45/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.6924 - val_loss: 0.5723 - val_accuracy: 0.6969\n",
            "Epoch 46/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7202 - val_loss: 0.5588 - val_accuracy: 0.6929\n",
            "Epoch 47/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7358 - val_loss: 0.5701 - val_accuracy: 0.7008\n",
            "Epoch 48/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7087 - val_loss: 0.5586 - val_accuracy: 0.6929\n",
            "Epoch 49/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7187 - val_loss: 0.5577 - val_accuracy: 0.7087\n",
            "Epoch 50/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.7278 - val_loss: 0.5569 - val_accuracy: 0.7165\n",
            "Epoch 51/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7434 - val_loss: 0.5579 - val_accuracy: 0.7047\n",
            "Epoch 52/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.7037 - val_loss: 0.5720 - val_accuracy: 0.6969\n",
            "Epoch 53/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7237 - val_loss: 0.5557 - val_accuracy: 0.7047\n",
            "Epoch 54/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7655 - val_loss: 0.5595 - val_accuracy: 0.7047\n",
            "Epoch 55/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.7366 - val_loss: 0.5775 - val_accuracy: 0.7047\n",
            "Epoch 56/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7562 - val_loss: 0.5516 - val_accuracy: 0.7165\n",
            "Epoch 57/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7370 - val_loss: 0.5565 - val_accuracy: 0.6890\n",
            "Epoch 58/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7198 - val_loss: 0.5515 - val_accuracy: 0.7205\n",
            "Epoch 59/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5622 - accuracy: 0.7169 - val_loss: 0.5467 - val_accuracy: 0.7165\n",
            "Epoch 60/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7269 - val_loss: 0.5465 - val_accuracy: 0.7165\n",
            "Epoch 61/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7396 - val_loss: 0.5459 - val_accuracy: 0.7323\n",
            "Epoch 62/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7378 - val_loss: 0.5481 - val_accuracy: 0.7323\n",
            "Epoch 63/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.6999 - val_loss: 0.5431 - val_accuracy: 0.7244\n",
            "Epoch 64/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7254 - val_loss: 0.5475 - val_accuracy: 0.7244\n",
            "Epoch 65/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.7457 - val_loss: 0.5486 - val_accuracy: 0.7283\n",
            "Epoch 66/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7533 - val_loss: 0.5436 - val_accuracy: 0.7283\n",
            "Epoch 67/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7916 - val_loss: 0.5489 - val_accuracy: 0.7520\n",
            "Epoch 68/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7196 - val_loss: 0.5441 - val_accuracy: 0.7283\n",
            "Epoch 69/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.7467 - val_loss: 0.5515 - val_accuracy: 0.7362\n",
            "Epoch 70/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7288 - val_loss: 0.5431 - val_accuracy: 0.7165\n",
            "Epoch 71/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7714 - val_loss: 0.5419 - val_accuracy: 0.7323\n",
            "Epoch 72/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7505 - val_loss: 0.6110 - val_accuracy: 0.6929\n",
            "Epoch 73/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7420 - val_loss: 0.5528 - val_accuracy: 0.7244\n",
            "Epoch 74/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5398 - accuracy: 0.7297 - val_loss: 0.5396 - val_accuracy: 0.7244\n",
            "Epoch 75/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7707 - val_loss: 0.5469 - val_accuracy: 0.7283\n",
            "Epoch 76/150\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7938 - val_loss: 0.5798 - val_accuracy: 0.6969\n",
            "Epoch 77/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7644 - val_loss: 0.5675 - val_accuracy: 0.7205\n",
            "Epoch 78/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7779 - val_loss: 0.5412 - val_accuracy: 0.7480\n",
            "Epoch 79/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7264 - val_loss: 0.5504 - val_accuracy: 0.7323\n",
            "Epoch 80/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5298 - accuracy: 0.7135 - val_loss: 0.5455 - val_accuracy: 0.7441\n",
            "Epoch 81/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7762 - val_loss: 0.5362 - val_accuracy: 0.7165\n",
            "Epoch 82/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7687 - val_loss: 0.5464 - val_accuracy: 0.7205\n",
            "Epoch 83/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7474 - val_loss: 0.5412 - val_accuracy: 0.7165\n",
            "Epoch 84/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.7458 - val_loss: 0.5448 - val_accuracy: 0.7087\n",
            "Epoch 85/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7615 - val_loss: 0.5389 - val_accuracy: 0.7126\n",
            "Epoch 86/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7952 - val_loss: 0.5382 - val_accuracy: 0.7362\n",
            "Epoch 87/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7630 - val_loss: 0.5635 - val_accuracy: 0.7126\n",
            "Epoch 88/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7761 - val_loss: 0.5455 - val_accuracy: 0.7402\n",
            "Epoch 89/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7612 - val_loss: 0.5347 - val_accuracy: 0.7205\n",
            "Epoch 90/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7881 - val_loss: 0.5594 - val_accuracy: 0.7165\n",
            "Epoch 91/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7549 - val_loss: 0.5468 - val_accuracy: 0.7165\n",
            "Epoch 92/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7625 - val_loss: 0.5340 - val_accuracy: 0.7402\n",
            "Epoch 93/150\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4831 - accuracy: 0.7665 - val_loss: 0.5331 - val_accuracy: 0.7323\n",
            "Epoch 94/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.7727 - val_loss: 0.5349 - val_accuracy: 0.7402\n",
            "Epoch 95/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7840 - val_loss: 0.5347 - val_accuracy: 0.7323\n",
            "Epoch 96/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7434 - val_loss: 0.5991 - val_accuracy: 0.7008\n",
            "Epoch 97/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7642 - val_loss: 0.5257 - val_accuracy: 0.7244\n",
            "Epoch 98/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7699 - val_loss: 0.5481 - val_accuracy: 0.7205\n",
            "Epoch 99/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7871 - val_loss: 0.5365 - val_accuracy: 0.7283\n",
            "Epoch 100/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7953 - val_loss: 0.5447 - val_accuracy: 0.7165\n",
            "Epoch 101/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7687 - val_loss: 0.5372 - val_accuracy: 0.7244\n",
            "Epoch 102/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7795 - val_loss: 0.5352 - val_accuracy: 0.7480\n",
            "Epoch 103/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7589 - val_loss: 0.5300 - val_accuracy: 0.7244\n",
            "Epoch 104/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7532 - val_loss: 0.5298 - val_accuracy: 0.7441\n",
            "Epoch 105/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7803 - val_loss: 0.5301 - val_accuracy: 0.7402\n",
            "Epoch 106/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7231 - val_loss: 0.5264 - val_accuracy: 0.7244\n",
            "Epoch 107/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7919 - val_loss: 0.5597 - val_accuracy: 0.7362\n",
            "Epoch 108/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.7564 - val_loss: 0.5249 - val_accuracy: 0.7402\n",
            "Epoch 109/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7778 - val_loss: 0.5304 - val_accuracy: 0.7717\n",
            "Epoch 110/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7592 - val_loss: 0.5308 - val_accuracy: 0.7598\n",
            "Epoch 111/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7652 - val_loss: 0.5242 - val_accuracy: 0.7480\n",
            "Epoch 112/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7761 - val_loss: 0.5311 - val_accuracy: 0.7323\n",
            "Epoch 113/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8076 - val_loss: 0.5343 - val_accuracy: 0.7559\n",
            "Epoch 114/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7573 - val_loss: 0.5368 - val_accuracy: 0.7402\n",
            "Epoch 115/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.7800 - val_loss: 0.5253 - val_accuracy: 0.7362\n",
            "Epoch 116/150\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.7951 - val_loss: 0.5244 - val_accuracy: 0.7638\n",
            "Epoch 117/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7858 - val_loss: 0.5238 - val_accuracy: 0.7520\n",
            "Epoch 118/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7733 - val_loss: 0.5267 - val_accuracy: 0.7559\n",
            "Epoch 119/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.7768 - val_loss: 0.5338 - val_accuracy: 0.7244\n",
            "Epoch 120/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.7726 - val_loss: 0.5258 - val_accuracy: 0.7520\n",
            "Epoch 121/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.7866 - val_loss: 0.5284 - val_accuracy: 0.7480\n",
            "Epoch 122/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7593 - val_loss: 0.5314 - val_accuracy: 0.7362\n",
            "Epoch 123/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7833 - val_loss: 0.5273 - val_accuracy: 0.7598\n",
            "Epoch 124/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.7909 - val_loss: 0.5340 - val_accuracy: 0.7559\n",
            "Epoch 125/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7722 - val_loss: 0.5533 - val_accuracy: 0.7244\n",
            "Epoch 126/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7613 - val_loss: 0.5501 - val_accuracy: 0.7205\n",
            "Epoch 127/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.7910 - val_loss: 0.5471 - val_accuracy: 0.7441\n",
            "Epoch 128/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7832 - val_loss: 0.5343 - val_accuracy: 0.7559\n",
            "Epoch 129/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.7789 - val_loss: 0.5308 - val_accuracy: 0.7598\n",
            "Epoch 130/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8088 - val_loss: 0.5238 - val_accuracy: 0.7480\n",
            "Epoch 131/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.7719 - val_loss: 0.5357 - val_accuracy: 0.7520\n",
            "Epoch 132/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7944 - val_loss: 0.5527 - val_accuracy: 0.7441\n",
            "Epoch 133/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.7677 - val_loss: 0.5498 - val_accuracy: 0.7402\n",
            "Epoch 134/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.7930 - val_loss: 0.5385 - val_accuracy: 0.7756\n",
            "Epoch 135/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7818 - val_loss: 0.5354 - val_accuracy: 0.7559\n",
            "Epoch 136/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7909 - val_loss: 0.5453 - val_accuracy: 0.7480\n",
            "Epoch 137/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.7804 - val_loss: 0.5276 - val_accuracy: 0.7677\n",
            "Epoch 138/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7692 - val_loss: 0.5301 - val_accuracy: 0.7441\n",
            "Epoch 139/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.7705 - val_loss: 0.5398 - val_accuracy: 0.7402\n",
            "Epoch 140/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.7772 - val_loss: 0.5390 - val_accuracy: 0.7441\n",
            "Epoch 141/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7815 - val_loss: 0.5272 - val_accuracy: 0.7520\n",
            "Epoch 142/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7870 - val_loss: 0.5352 - val_accuracy: 0.7638\n",
            "Epoch 143/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.7858 - val_loss: 0.5323 - val_accuracy: 0.7598\n",
            "Epoch 144/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.7762 - val_loss: 0.5215 - val_accuracy: 0.7638\n",
            "Epoch 145/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7737 - val_loss: 0.5201 - val_accuracy: 0.7638\n",
            "Epoch 146/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.7839 - val_loss: 0.5487 - val_accuracy: 0.7205\n",
            "Epoch 147/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.7704 - val_loss: 0.5596 - val_accuracy: 0.7362\n",
            "Epoch 148/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.7997 - val_loss: 0.5283 - val_accuracy: 0.7717\n",
            "Epoch 149/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.7709 - val_loss: 0.5244 - val_accuracy: 0.7638\n",
            "Epoch 150/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7913 - val_loss: 0.5269 - val_accuracy: 0.7559\n",
            "24/24 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7760\n",
            "accuracy: 77.60%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcWMVmtsRI5J",
        "outputId": "31302c31-7517-4b0b-cfc9-47d16a3fe9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(X, Y):\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(8, kernel_initializer='uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(1, kernel_initializer='uniform' , activation= 'sigmoid' ))\n",
        "  # Compile model\n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "  # Fit the model\n",
        "  model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 75.32%\n",
            "accuracy: 80.52%\n",
            "accuracy: 75.32%\n",
            "WARNING:tensorflow:5 out of the last 34 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4ba0daef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 81.82%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4bd66e9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 67.53%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4b12fc7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 64.94%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4b0151560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 81.82%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4ae6fdb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 64.94%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4ad60ab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 72.37%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa4ac488a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 76.32%\n",
            "74.09% (+/- 6.18%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO8xI3dIbLDS"
      },
      "source": [
        "#**Keras Models With Scikit-Learn For General Machine Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_3B4YJCb3bS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec382a6-2bd0-4e8d-b9fa-696e08627028"
      },
      "source": [
        "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
        "  model.add(Dense(1, kernel_initializer= 'uniform' , activation= 'sigmoid' ))\n",
        "  # Compile model\n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "  return model\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10, verbose=0)\n",
        "# evaluate using 10-fold cross validation\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6510594606399536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtrUea2QNx9M"
      },
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer= 'rmsprop' , init= 'glorot_uniform' ):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "init = [ 'glorot_uniform' , 'normal' , 'uniform' ]\n",
        "optimizers = [ 'rmsprop' , 'adam' ]\n",
        "#param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batch_size, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "pdeep = grid_result.get_params([True])\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r %r\" % (mean, stdev, param,pdeep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdv17LDBC9T"
      },
      "source": [
        "# **Multiclass Classification with the Iris Flowers Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ3wDt1eA7f1",
        "outputId": "cba8c790-661a-4b97-c776-14326a4c0152"
      },
      "source": [
        "# multi-class classification with Keras\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"iris.data.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y = dataset[:,4]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\n",
        " \n",
        "# define baseline model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0667029b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065effba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ecee840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065edf87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ec51730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065e93b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ea102f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065e8f7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065ed46ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f06626ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 98.67% (2.67%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmhC1biRHcpV",
        "outputId": "248c6951-4608-4b9e-d50a-bde3e7359b35"
      },
      "source": [
        "# Binary Classification with Sonar Dataset: Baseline\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# load dataset\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# baseline model\n",
        "def create_baseline():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# evaluate model with standardized dataset\n",
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f065b38e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 83.14% (5.86%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVTKnzwCIC-i",
        "outputId": "e816253f-121d-4c8b-c0cc-f4d31bbeddfa"
      },
      "source": [
        "# Binary Classification with Sonar Dataset: Standardized\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# baseline model\n",
        "def create_baseline():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# evaluate baseline model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: 84.62% (7.30%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdG3s9Pz-lTV",
        "outputId": "36b747ba-bc85-47bb-cc32-db58b247f720"
      },
      "source": [
        "\n",
        "# Regression Example With Boston Dataset: Baseline\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model\n",
        "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
        "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: -29.26 (23.29) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Uu5lOf4_U-7",
        "outputId": "ba12532f-c882-4bb9-acf4-7dfb2d8fd4f9"
      },
      "source": [
        "\n",
        "# Regression Example With Boston Dataset: Standardized\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: -31.48 (30.93) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXFKvYIK_-Wa",
        "outputId": "86793a7e-f481-4d8d-efa4-1cf573d34117"
      },
      "source": [
        "\n",
        "# Regression Example With Boston Dataset: Standardized and Larger\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define the model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger: -22.39 (24.13) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFpfdTOZ9rix",
        "outputId": "bbcd8dbb-a17d-434e-f644-55605c5f6e33"
      },
      "source": [
        "\n",
        "# Regression Example With Boston Dataset: Standardized and Wider\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "# define wider model\n",
        "def wider_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wider: -22.37 (26.53) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUpPyWnFASzB",
        "outputId": "18496528-8029-423e-88c5-be474f036fb6"
      },
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "import os\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        " \n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        " \n",
        "# later...\n",
        " \n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 72.01%\n",
            "Saved model to disk\n",
            "Loaded model from disk\n",
            "accuracy: 72.01%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0SOhlvkBMRQ",
        "outputId": "9619864f-8e58-4ca9-8835-ee48aae566ac"
      },
      "source": [
        "\n",
        "# MLP for Pima Indians Dataset serialize to YAML and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_yaml\n",
        "import numpy\n",
        "import os\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        " \n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        " \n",
        "# later...\n",
        " \n",
        "# load YAML and create model\n",
        "yaml_file = open('model.yaml', 'r')\n",
        "loaded_model_yaml = yaml_file.read()\n",
        "yaml_file.close()\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 76.95%\n",
            "Saved model to disk\n",
            "Loaded model from disk\n",
            "accuracy: 76.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6lnZK2HB7mJ",
        "outputId": "4cbb7559-e00d-4ba5-9212-b2721cbc48f5"
      },
      "source": [
        "\n",
        "# MLP for Pima Indians Dataset saved to single file\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# load pima indians dataset\n",
        "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "# save model and architecture to single file\n",
        "model.save(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "# load and evaluate a saved model\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# summarize model.\n",
        "model.summary()\n",
        "# load dataset\n",
        "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# evaluate the model\n",
        "score = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 76.82%\n",
            "Saved model to disk\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_38 (Dense)             (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 221\n",
            "Trainable params: 221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "accuracy: 76.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UneDY0NLybj"
      },
      "source": [
        "# Checkpoint the weights when validation accuracy improves\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEeBRJqpMmjk"
      },
      "source": [
        "# Checkpoint the weights for best model on validation accuracy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# checkpoint\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFoGQa-MNPnS",
        "outputId": "f15726bf-7b32-483c-f3ce-283e92478dfb"
      },
      "source": [
        "# How to load and use weights from a checkpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# load weights\n",
        "model.load_weights(\"weights.best.hdf5\")\n",
        "# Compile model (required to make predictions)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Created model and loaded weights from file\")\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created model and loaded weights from file\n",
            "accuracy: 75.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDAwpwJP8Al"
      },
      "source": [
        "# Visualize training history\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhF6x3npSqaq",
        "outputId": "11195cc8-52ae-4056-ae29-9a5f05316921"
      },
      "source": [
        "# Baseline Model on the Sonar Dataset\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        " \n",
        "# baseline\n",
        "def create_baseline():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\n",
        "\tmodel.add(Dense(30,  activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tsgd = SGD(lr=0.01, momentum=0.8)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 3907 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454d3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 3909 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa48462d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454e59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a53ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a3cbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a2e6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa453c9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Baseline: 86.48% (8.44%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYj0izr6S-Dk",
        "outputId": "2d91d9bc-7671-4dc5-c377-895b318acf0a"
      },
      "source": [
        "# Example of Dropout on the Sonar Dataset: Visible Layer\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        " \n",
        "# dropout in the input layer with weight constraint\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dropout(0.2, input_shape=(60,)))\n",
        "\tmodel.add(Dense(60, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tsgd = SGD(lr=0.1, momentum=0.9)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451f9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa455d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa453c91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454e06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451a1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451a1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45595ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4a45d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45472488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45472c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Visible: 90.40% (6.45%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP6i_g1ZTXXN",
        "outputId": "ffde770b-2237-443f-f3a0-7abc87a468da"
      },
      "source": [
        "# Example of Dropout on the Sonar Dataset: Hidden Layer\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "# load dataset\n",
        "dataframe = read_csv(\"sonar.all-data.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        " \n",
        "# dropout in hidden layers with weight constraint\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(30, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tsgd = SGD(lr=0.1, momentum=0.9)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4bc4b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa4b3660d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451deae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451de0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451de048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa454e5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa452991e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa45595048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa3ae1a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7faa451a1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Hidden: 81.69% (7.73%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG76_cBLV0GL"
      },
      "source": [
        "# Time Based Learning Rate Decay\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load dataset\n",
        "dataframe = read_csv(\"ionosphere.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:34].astype(float)\n",
        "Y = dataset[:,34]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(34, input_dim=34, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "epochs = 50\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN_bKSqDXpF1",
        "outputId": "18babe37-2e1e-4b17-a1ea-63d36b6b686e"
      },
      "source": [
        "# Drop-Based Learning Rate Decay\n",
        "from pandas import read_csv\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import LearningRateScheduler\n",
        " \n",
        "# learning rate schedule\n",
        "def step_decay(epoch):\n",
        "\tinitial_lrate = 0.1\n",
        "\tdrop = 0.5\n",
        "\tepochs_drop = 10.0\n",
        "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "\treturn lrate\n",
        " \n",
        "# load dataset\n",
        "dataframe = read_csv(\"ionosphere.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:34].astype(float)\n",
        "Y = dataset[:,34]\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(34, input_dim=34, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "sgd = SGD(lr=0.0, momentum=0.9)\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "# learning schedule callback\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "callbacks_list = [lrate]\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9/9 - 1s - loss: 0.6434 - accuracy: 0.5702 - val_loss: 0.4615 - val_accuracy: 0.8621\n",
            "Epoch 2/50\n",
            "9/9 - 0s - loss: 0.3983 - accuracy: 0.8553 - val_loss: 0.1839 - val_accuracy: 0.9655\n",
            "Epoch 3/50\n",
            "9/9 - 0s - loss: 0.3332 - accuracy: 0.8723 - val_loss: 0.3178 - val_accuracy: 0.9310\n",
            "Epoch 4/50\n",
            "9/9 - 0s - loss: 0.2141 - accuracy: 0.9191 - val_loss: 0.1151 - val_accuracy: 0.9741\n",
            "Epoch 5/50\n",
            "9/9 - 0s - loss: 0.1660 - accuracy: 0.9447 - val_loss: 0.1624 - val_accuracy: 0.9741\n",
            "Epoch 6/50\n",
            "9/9 - 0s - loss: 0.1484 - accuracy: 0.9574 - val_loss: 0.1003 - val_accuracy: 0.9828\n",
            "Epoch 7/50\n",
            "9/9 - 0s - loss: 0.1280 - accuracy: 0.9702 - val_loss: 0.1223 - val_accuracy: 0.9828\n",
            "Epoch 8/50\n",
            "9/9 - 0s - loss: 0.1116 - accuracy: 0.9660 - val_loss: 0.0978 - val_accuracy: 0.9914\n",
            "Epoch 9/50\n",
            "9/9 - 0s - loss: 0.1006 - accuracy: 0.9745 - val_loss: 0.0631 - val_accuracy: 0.9914\n",
            "Epoch 10/50\n",
            "9/9 - 0s - loss: 0.0867 - accuracy: 0.9830 - val_loss: 0.0843 - val_accuracy: 0.9914\n",
            "Epoch 11/50\n",
            "9/9 - 0s - loss: 0.0815 - accuracy: 0.9830 - val_loss: 0.0680 - val_accuracy: 0.9914\n",
            "Epoch 12/50\n",
            "9/9 - 0s - loss: 0.0766 - accuracy: 0.9830 - val_loss: 0.0741 - val_accuracy: 0.9914\n",
            "Epoch 13/50\n",
            "9/9 - 0s - loss: 0.0730 - accuracy: 0.9830 - val_loss: 0.0687 - val_accuracy: 0.9914\n",
            "Epoch 14/50\n",
            "9/9 - 0s - loss: 0.0692 - accuracy: 0.9830 - val_loss: 0.0697 - val_accuracy: 0.9914\n",
            "Epoch 15/50\n",
            "9/9 - 0s - loss: 0.0658 - accuracy: 0.9830 - val_loss: 0.0712 - val_accuracy: 0.9914\n",
            "Epoch 16/50\n",
            "9/9 - 0s - loss: 0.0663 - accuracy: 0.9830 - val_loss: 0.0625 - val_accuracy: 0.9914\n",
            "Epoch 17/50\n",
            "9/9 - 0s - loss: 0.0627 - accuracy: 0.9830 - val_loss: 0.0710 - val_accuracy: 0.9914\n",
            "Epoch 18/50\n",
            "9/9 - 0s - loss: 0.0644 - accuracy: 0.9830 - val_loss: 0.0567 - val_accuracy: 0.9914\n",
            "Epoch 19/50\n",
            "9/9 - 0s - loss: 0.0645 - accuracy: 0.9830 - val_loss: 0.0550 - val_accuracy: 0.9914\n",
            "Epoch 20/50\n",
            "9/9 - 0s - loss: 0.0682 - accuracy: 0.9830 - val_loss: 0.1024 - val_accuracy: 0.9828\n",
            "Epoch 21/50\n",
            "9/9 - 0s - loss: 0.0605 - accuracy: 0.9830 - val_loss: 0.0472 - val_accuracy: 0.9914\n",
            "Epoch 22/50\n",
            "9/9 - 0s - loss: 0.0563 - accuracy: 0.9830 - val_loss: 0.0567 - val_accuracy: 0.9914\n",
            "Epoch 23/50\n",
            "9/9 - 0s - loss: 0.0564 - accuracy: 0.9830 - val_loss: 0.0710 - val_accuracy: 0.9914\n",
            "Epoch 24/50\n",
            "9/9 - 0s - loss: 0.0510 - accuracy: 0.9830 - val_loss: 0.0493 - val_accuracy: 0.9914\n",
            "Epoch 25/50\n",
            "9/9 - 0s - loss: 0.0529 - accuracy: 0.9830 - val_loss: 0.0513 - val_accuracy: 0.9914\n",
            "Epoch 26/50\n",
            "9/9 - 0s - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.0644 - val_accuracy: 0.9914\n",
            "Epoch 27/50\n",
            "9/9 - 0s - loss: 0.0486 - accuracy: 0.9830 - val_loss: 0.0588 - val_accuracy: 0.9914\n",
            "Epoch 28/50\n",
            "9/9 - 0s - loss: 0.0475 - accuracy: 0.9830 - val_loss: 0.0534 - val_accuracy: 0.9914\n",
            "Epoch 29/50\n",
            "9/9 - 0s - loss: 0.0474 - accuracy: 0.9830 - val_loss: 0.0569 - val_accuracy: 0.9914\n",
            "Epoch 30/50\n",
            "9/9 - 0s - loss: 0.0472 - accuracy: 0.9830 - val_loss: 0.0489 - val_accuracy: 0.9914\n",
            "Epoch 31/50\n",
            "9/9 - 0s - loss: 0.0460 - accuracy: 0.9830 - val_loss: 0.0509 - val_accuracy: 0.9914\n",
            "Epoch 32/50\n",
            "9/9 - 0s - loss: 0.0454 - accuracy: 0.9872 - val_loss: 0.0555 - val_accuracy: 0.9914\n",
            "Epoch 33/50\n",
            "9/9 - 0s - loss: 0.0453 - accuracy: 0.9872 - val_loss: 0.0538 - val_accuracy: 0.9914\n",
            "Epoch 34/50\n",
            "9/9 - 0s - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0549 - val_accuracy: 0.9914\n",
            "Epoch 35/50\n",
            "9/9 - 0s - loss: 0.0442 - accuracy: 0.9872 - val_loss: 0.0536 - val_accuracy: 0.9914\n",
            "Epoch 36/50\n",
            "9/9 - 0s - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0561 - val_accuracy: 0.9914\n",
            "Epoch 37/50\n",
            "9/9 - 0s - loss: 0.0436 - accuracy: 0.9872 - val_loss: 0.0530 - val_accuracy: 0.9914\n",
            "Epoch 38/50\n",
            "9/9 - 0s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0532 - val_accuracy: 0.9914\n",
            "Epoch 39/50\n",
            "9/9 - 0s - loss: 0.0430 - accuracy: 0.9872 - val_loss: 0.0536 - val_accuracy: 0.9914\n",
            "Epoch 40/50\n",
            "9/9 - 0s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0515 - val_accuracy: 0.9914\n",
            "Epoch 41/50\n",
            "9/9 - 0s - loss: 0.0435 - accuracy: 0.9872 - val_loss: 0.0592 - val_accuracy: 0.9914\n",
            "Epoch 42/50\n",
            "9/9 - 0s - loss: 0.0426 - accuracy: 0.9915 - val_loss: 0.0575 - val_accuracy: 0.9914\n",
            "Epoch 43/50\n",
            "9/9 - 0s - loss: 0.0423 - accuracy: 0.9915 - val_loss: 0.0566 - val_accuracy: 0.9914\n",
            "Epoch 44/50\n",
            "9/9 - 0s - loss: 0.0418 - accuracy: 0.9915 - val_loss: 0.0538 - val_accuracy: 0.9914\n",
            "Epoch 45/50\n",
            "9/9 - 0s - loss: 0.0418 - accuracy: 0.9872 - val_loss: 0.0523 - val_accuracy: 0.9914\n",
            "Epoch 46/50\n",
            "9/9 - 0s - loss: 0.0417 - accuracy: 0.9872 - val_loss: 0.0526 - val_accuracy: 0.9914\n",
            "Epoch 47/50\n",
            "9/9 - 0s - loss: 0.0415 - accuracy: 0.9872 - val_loss: 0.0521 - val_accuracy: 0.9914\n",
            "Epoch 48/50\n",
            "9/9 - 0s - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0517 - val_accuracy: 0.9914\n",
            "Epoch 49/50\n",
            "9/9 - 0s - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0531 - val_accuracy: 0.9914\n",
            "Epoch 50/50\n",
            "9/9 - 0s - loss: 0.0411 - accuracy: 0.9872 - val_loss: 0.0526 - val_accuracy: 0.9914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faa4a447518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}