{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningWithPyTorch",
      "provenance": [],
      "authorship_tag": "ABX9TyO0kigKL8jddxzhOfYi7mtD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athiagarajan/JBLeeFiles/blob/master/DeepLearningWithPyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2TpCS2Xm4E"
      },
      "source": [
        "https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhdG5BZA4zYi",
        "outputId": "66b2dce6-9c98-4e76-d9c6-6df6d23fd055"
      },
      "source": [
        "# pytorch mlp for binary classification\r\n",
        "# https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\r\n",
        "from numpy import vstack\r\n",
        "from pandas import read_csv\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import random_split\r\n",
        "from torch import Tensor\r\n",
        "from torch.nn import Linear\r\n",
        "from torch.nn import ReLU\r\n",
        "from torch.nn import Sigmoid\r\n",
        "from torch.nn import Module\r\n",
        "from torch.optim import SGD\r\n",
        "from torch.nn import BCELoss\r\n",
        "from torch.nn.init import kaiming_uniform_\r\n",
        "from torch.nn.init import xavier_uniform_\r\n",
        " \r\n",
        "# dataset definition\r\n",
        "class CSVDataset(Dataset):\r\n",
        "    # load the dataset\r\n",
        "    def __init__(self, path):\r\n",
        "        # load the csv file as a dataframe\r\n",
        "        df = read_csv(path, header=None)\r\n",
        "        # store the inputs and outputs\r\n",
        "        self.X = df.values[:, :-1]\r\n",
        "        self.y = df.values[:, -1]\r\n",
        "        # ensure input data is floats\r\n",
        "        self.X = self.X.astype('float32')\r\n",
        "        # label encode target and ensure the values are floats\r\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\r\n",
        "        self.y = self.y.astype('float32')\r\n",
        "        self.y = self.y.reshape((len(self.y), 1))\r\n",
        " \r\n",
        "    # number of rows in the dataset\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.X)\r\n",
        " \r\n",
        "    # get a row at an index\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return [self.X[idx], self.y[idx]]\r\n",
        " \r\n",
        "    # get indexes for train and test rows\r\n",
        "    def get_splits(self, n_test=0.33):\r\n",
        "        # determine sizes\r\n",
        "        test_size = round(n_test * len(self.X))\r\n",
        "        train_size = len(self.X) - test_size\r\n",
        "        # calculate the split\r\n",
        "        return random_split(self, [train_size, test_size])\r\n",
        " \r\n",
        "# model definition\r\n",
        "class MLP(Module):\r\n",
        "    # define model elements\r\n",
        "    def __init__(self, n_inputs):\r\n",
        "        super(MLP, self).__init__()\r\n",
        "        # input to first hidden layer\r\n",
        "        self.hidden1 = Linear(n_inputs, 10)\r\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\r\n",
        "        self.act1 = ReLU()\r\n",
        "        # second hidden layer\r\n",
        "        self.hidden2 = Linear(10, 8)\r\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\r\n",
        "        self.act2 = ReLU()\r\n",
        "        # third hidden layer and output\r\n",
        "        self.hidden3 = Linear(8, 1)\r\n",
        "        xavier_uniform_(self.hidden3.weight)\r\n",
        "        self.act3 = Sigmoid()\r\n",
        " \r\n",
        "    # forward propagate input\r\n",
        "    def forward(self, X):\r\n",
        "        # input to first hidden layer\r\n",
        "        X = self.hidden1(X)\r\n",
        "        X = self.act1(X)\r\n",
        "         # second hidden layer\r\n",
        "        X = self.hidden2(X)\r\n",
        "        X = self.act2(X)\r\n",
        "        # third hidden layer and output\r\n",
        "        X = self.hidden3(X)\r\n",
        "        X = self.act3(X)\r\n",
        "        return X\r\n",
        " \r\n",
        "# prepare the dataset\r\n",
        "def prepare_data(path):\r\n",
        "    # load the dataset\r\n",
        "    dataset = CSVDataset(path)\r\n",
        "    # calculate split\r\n",
        "    train, test = dataset.get_splits()\r\n",
        "    # prepare data loaders\r\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\r\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\r\n",
        "    return train_dl, test_dl\r\n",
        " \r\n",
        "# train the model\r\n",
        "def train_model(train_dl, model):\r\n",
        "    # define the optimization\r\n",
        "    criterion = BCELoss()\r\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
        "    # enumerate epochs\r\n",
        "    for epoch in range(100):\r\n",
        "        # enumerate mini batches\r\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\r\n",
        "            # clear the gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "            # compute the model output\r\n",
        "            yhat = model(inputs)\r\n",
        "            # calculate loss\r\n",
        "            loss = criterion(yhat, targets)\r\n",
        "            # credit assignment\r\n",
        "            loss.backward()\r\n",
        "            # update model weights\r\n",
        "            optimizer.step()\r\n",
        " \r\n",
        "# evaluate the model\r\n",
        "def evaluate_model(test_dl, model):\r\n",
        "    predictions, actuals = list(), list()\r\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\r\n",
        "        # evaluate the model on the test set\r\n",
        "        yhat = model(inputs)\r\n",
        "        # retrieve numpy array\r\n",
        "        yhat = yhat.detach().numpy()\r\n",
        "        actual = targets.numpy()\r\n",
        "        actual = actual.reshape((len(actual), 1))\r\n",
        "        # round to class values\r\n",
        "        yhat = yhat.round()\r\n",
        "        # store\r\n",
        "        predictions.append(yhat)\r\n",
        "        actuals.append(actual)\r\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\r\n",
        "    # calculate accuracy\r\n",
        "    acc = accuracy_score(actuals, predictions)\r\n",
        "    return acc\r\n",
        " \r\n",
        "# make a class prediction for one row of data\r\n",
        "def predict(row, model):\r\n",
        "    # convert row to data\r\n",
        "    row = Tensor([row])\r\n",
        "    # make prediction\r\n",
        "    yhat = model(row)\r\n",
        "    # retrieve numpy array\r\n",
        "    yhat = yhat.detach().numpy()\r\n",
        "    return yhat\r\n",
        " \r\n",
        "# prepare the data\r\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\r\n",
        "train_dl, test_dl = prepare_data(path)\r\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\r\n",
        "# define the network\r\n",
        "model = MLP(34)\r\n",
        "# train the model\r\n",
        "train_model(train_dl, model)\r\n",
        "# evaluate the model\r\n",
        "acc = evaluate_model(test_dl, model)\r\n",
        "print('Accuracy: %.3f' % acc)\r\n",
        "# make a single prediction (expect class=1)\r\n",
        "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\r\n",
        "yhat = predict(row, model)\r\n",
        "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235 116\n",
            "Accuracy: 0.922\n",
            "Predicted: 0.999 (class=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3rvHKLLShUm",
        "outputId": "cb213838-b62b-4d03-db55-e7800cd5100d"
      },
      "source": [
        "# pytorch mlp for multiclass classification\r\n",
        "from numpy import vstack\r\n",
        "from numpy import argmax\r\n",
        "from pandas import read_csv\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import random_split\r\n",
        "from torch.nn import Linear\r\n",
        "from torch.nn import ReLU\r\n",
        "from torch.nn import Softmax\r\n",
        "from torch.nn import Module\r\n",
        "from torch.optim import SGD\r\n",
        "from torch.nn import CrossEntropyLoss\r\n",
        "from torch.nn.init import kaiming_uniform_\r\n",
        "from torch.nn.init import xavier_uniform_\r\n",
        " \r\n",
        "# dataset definition\r\n",
        "class CSVDataset(Dataset):\r\n",
        "    # load the dataset\r\n",
        "    def __init__(self, path):\r\n",
        "        # load the csv file as a dataframe\r\n",
        "        df = read_csv(path, header=None)\r\n",
        "        # store the inputs and outputs\r\n",
        "        self.X = df.values[:, :-1]\r\n",
        "        self.y = df.values[:, -1]\r\n",
        "        # ensure input data is floats\r\n",
        "        self.X = self.X.astype('float32')\r\n",
        "        # label encode target and ensure the values are floats\r\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\r\n",
        " \r\n",
        "    # number of rows in the dataset\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.X)\r\n",
        " \r\n",
        "    # get a row at an index\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return [self.X[idx], self.y[idx]]\r\n",
        " \r\n",
        "    # get indexes for train and test rows\r\n",
        "    def get_splits(self, n_test=0.33):\r\n",
        "        # determine sizes\r\n",
        "        test_size = round(n_test * len(self.X))\r\n",
        "        train_size = len(self.X) - test_size\r\n",
        "        # calculate the split\r\n",
        "        return random_split(self, [train_size, test_size])\r\n",
        " \r\n",
        "# model definition\r\n",
        "class MLP(Module):\r\n",
        "    # define model elements\r\n",
        "    def __init__(self, n_inputs):\r\n",
        "        super(MLP, self).__init__()\r\n",
        "        # input to first hidden layer\r\n",
        "        self.hidden1 = Linear(n_inputs, 10)\r\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\r\n",
        "        self.act1 = ReLU()\r\n",
        "        # second hidden layer\r\n",
        "        self.hidden2 = Linear(10, 8)\r\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\r\n",
        "        self.act2 = ReLU()\r\n",
        "        # third hidden layer and output\r\n",
        "        self.hidden3 = Linear(8, 3)\r\n",
        "        xavier_uniform_(self.hidden3.weight)\r\n",
        "        self.act3 = Softmax(dim=1)\r\n",
        " \r\n",
        "    # forward propagate input\r\n",
        "    def forward(self, X):\r\n",
        "        # input to first hidden layer\r\n",
        "        X = self.hidden1(X)\r\n",
        "        X = self.act1(X)\r\n",
        "        # second hidden layer\r\n",
        "        X = self.hidden2(X)\r\n",
        "        X = self.act2(X)\r\n",
        "        # output layer\r\n",
        "        X = self.hidden3(X)\r\n",
        "        X = self.act3(X)\r\n",
        "        return X\r\n",
        " \r\n",
        "# prepare the dataset\r\n",
        "def prepare_data(path):\r\n",
        "    # load the dataset\r\n",
        "    dataset = CSVDataset(path)\r\n",
        "    # calculate split\r\n",
        "    train, test = dataset.get_splits()\r\n",
        "    # prepare data loaders\r\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\r\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\r\n",
        "    return train_dl, test_dl\r\n",
        " \r\n",
        "# train the model\r\n",
        "def train_model(train_dl, model):\r\n",
        "    # define the optimization\r\n",
        "    criterion = CrossEntropyLoss()\r\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
        "    # enumerate epochs\r\n",
        "    for epoch in range(500):\r\n",
        "        # enumerate mini batches\r\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\r\n",
        "            # clear the gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "            # compute the model output\r\n",
        "            yhat = model(inputs)\r\n",
        "            # calculate loss\r\n",
        "            loss = criterion(yhat, targets)\r\n",
        "            # credit assignment\r\n",
        "            loss.backward()\r\n",
        "            # update model weights\r\n",
        "            optimizer.step()\r\n",
        " \r\n",
        "# evaluate the model\r\n",
        "def evaluate_model(test_dl, model):\r\n",
        "    predictions, actuals = list(), list()\r\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\r\n",
        "        # evaluate the model on the test set\r\n",
        "        yhat = model(inputs)\r\n",
        "        # retrieve numpy array\r\n",
        "        yhat = yhat.detach().numpy()\r\n",
        "        actual = targets.numpy()\r\n",
        "        # convert to class labels\r\n",
        "        yhat = argmax(yhat, axis=1)\r\n",
        "        # reshape for stacking\r\n",
        "        actual = actual.reshape((len(actual), 1))\r\n",
        "        yhat = yhat.reshape((len(yhat), 1))\r\n",
        "        # store\r\n",
        "        predictions.append(yhat)\r\n",
        "        actuals.append(actual)\r\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\r\n",
        "    # calculate accuracy\r\n",
        "    acc = accuracy_score(actuals, predictions)\r\n",
        "    return acc\r\n",
        " \r\n",
        "# make a class prediction for one row of data\r\n",
        "def predict(row, model):\r\n",
        "    # convert row to data\r\n",
        "    row = Tensor([row])\r\n",
        "    # make prediction\r\n",
        "    yhat = model(row)\r\n",
        "    # retrieve numpy array\r\n",
        "    yhat = yhat.detach().numpy()\r\n",
        "    return yhat\r\n",
        " \r\n",
        "# prepare the data\r\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\r\n",
        "train_dl, test_dl = prepare_data(path)\r\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\r\n",
        "# define the network\r\n",
        "model = MLP(4)\r\n",
        "# train the model\r\n",
        "train_model(train_dl, model)\r\n",
        "# evaluate the model\r\n",
        "acc = evaluate_model(test_dl, model)\r\n",
        "print('Accuracy: %.3f' % acc)\r\n",
        "# make a single prediction\r\n",
        "row = [5.1,3.5,1.4,0.2]\r\n",
        "yhat = predict(row, model)\r\n",
        "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 50\n",
            "Accuracy: 0.960\n",
            "Predicted: [[9.9992204e-01 7.7914410e-05 3.2391045e-34]] (class=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iut9KK0UU2aw",
        "outputId": "132fba20-0e0f-401a-8115-e7c73efc471e"
      },
      "source": [
        "# pytorch mlp for regression\r\n",
        "from numpy import vstack\r\n",
        "from numpy import sqrt\r\n",
        "from pandas import read_csv\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data import random_split\r\n",
        "from torch import Tensor\r\n",
        "from torch.nn import Linear\r\n",
        "from torch.nn import Sigmoid\r\n",
        "from torch.nn import Module\r\n",
        "from torch.optim import SGD\r\n",
        "from torch.nn import MSELoss\r\n",
        "from torch.nn.init import xavier_uniform_\r\n",
        " \r\n",
        "# dataset definition\r\n",
        "class CSVDataset(Dataset):\r\n",
        "    # load the dataset\r\n",
        "    def __init__(self, path):\r\n",
        "        # load the csv file as a dataframe\r\n",
        "        df = read_csv(path, header=None)\r\n",
        "        # store the inputs and outputs\r\n",
        "        self.X = df.values[:, :-1].astype('float32')\r\n",
        "        self.y = df.values[:, -1].astype('float32')\r\n",
        "        # ensure target has the right shape\r\n",
        "        self.y = self.y.reshape((len(self.y), 1))\r\n",
        " \r\n",
        "    # number of rows in the dataset\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.X)\r\n",
        " \r\n",
        "    # get a row at an index\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return [self.X[idx], self.y[idx]]\r\n",
        " \r\n",
        "    # get indexes for train and test rows\r\n",
        "    def get_splits(self, n_test=0.33):\r\n",
        "        # determine sizes\r\n",
        "        test_size = round(n_test * len(self.X))\r\n",
        "        train_size = len(self.X) - test_size\r\n",
        "        # calculate the split\r\n",
        "        return random_split(self, [train_size, test_size])\r\n",
        " \r\n",
        "# model definition\r\n",
        "class MLP(Module):\r\n",
        "    # define model elements\r\n",
        "    def __init__(self, n_inputs):\r\n",
        "        super(MLP, self).__init__()\r\n",
        "        # input to first hidden layer\r\n",
        "        self.hidden1 = Linear(n_inputs, 10)\r\n",
        "        xavier_uniform_(self.hidden1.weight)\r\n",
        "        self.act1 = Sigmoid()\r\n",
        "        # second hidden layer\r\n",
        "        self.hidden2 = Linear(10, 8)\r\n",
        "        xavier_uniform_(self.hidden2.weight)\r\n",
        "        self.act2 = Sigmoid()\r\n",
        "        # third hidden layer and output\r\n",
        "        self.hidden3 = Linear(8, 1)\r\n",
        "        xavier_uniform_(self.hidden3.weight)\r\n",
        " \r\n",
        "    # forward propagate input\r\n",
        "    def forward(self, X):\r\n",
        "        # input to first hidden layer\r\n",
        "        X = self.hidden1(X)\r\n",
        "        X = self.act1(X)\r\n",
        "         # second hidden layer\r\n",
        "        X = self.hidden2(X)\r\n",
        "        X = self.act2(X)\r\n",
        "        # third hidden layer and output\r\n",
        "        X = self.hidden3(X)\r\n",
        "        return X\r\n",
        " \r\n",
        "# prepare the dataset\r\n",
        "def prepare_data(path):\r\n",
        "    # load the dataset\r\n",
        "    dataset = CSVDataset(path)\r\n",
        "    # calculate split\r\n",
        "    train, test = dataset.get_splits()\r\n",
        "    # prepare data loaders\r\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\r\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\r\n",
        "    return train_dl, test_dl\r\n",
        " \r\n",
        "# train the model\r\n",
        "def train_model(train_dl, model):\r\n",
        "    # define the optimization\r\n",
        "    criterion = MSELoss()\r\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
        "    # enumerate epochs\r\n",
        "    for epoch in range(100):\r\n",
        "        # enumerate mini batches\r\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\r\n",
        "            # clear the gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "            # compute the model output\r\n",
        "            yhat = model(inputs)\r\n",
        "            # calculate loss\r\n",
        "            loss = criterion(yhat, targets)\r\n",
        "            # credit assignment\r\n",
        "            loss.backward()\r\n",
        "            # update model weights\r\n",
        "            optimizer.step()\r\n",
        " \r\n",
        "# evaluate the model\r\n",
        "def evaluate_model(test_dl, model):\r\n",
        "    predictions, actuals = list(), list()\r\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\r\n",
        "        # evaluate the model on the test set\r\n",
        "        yhat = model(inputs)\r\n",
        "        # retrieve numpy array\r\n",
        "        yhat = yhat.detach().numpy()\r\n",
        "        actual = targets.numpy()\r\n",
        "        actual = actual.reshape((len(actual), 1))\r\n",
        "        # store\r\n",
        "        predictions.append(yhat)\r\n",
        "        actuals.append(actual)\r\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\r\n",
        "    # calculate mse\r\n",
        "    mse = mean_squared_error(actuals, predictions)\r\n",
        "    return mse\r\n",
        " \r\n",
        "# make a class prediction for one row of data\r\n",
        "def predict(row, model):\r\n",
        "    # convert row to data\r\n",
        "    row = Tensor([row])\r\n",
        "    # make prediction\r\n",
        "    yhat = model(row)\r\n",
        "    # retrieve numpy array\r\n",
        "    yhat = yhat.detach().numpy()\r\n",
        "    return yhat\r\n",
        " \r\n",
        "# prepare the data\r\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\r\n",
        "train_dl, test_dl = prepare_data(path)\r\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\r\n",
        "# define the network\r\n",
        "model = MLP(13)\r\n",
        "# train the model\r\n",
        "train_model(train_dl, model)\r\n",
        "# evaluate the model\r\n",
        "mse = evaluate_model(test_dl, model)\r\n",
        "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\r\n",
        "# make a single prediction (expect class=1)\r\n",
        "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\r\n",
        "yhat = predict(row, model)\r\n",
        "print('Predicted: %.3f' % yhat)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "339 167\n",
            "MSE: 76.494, RMSE: 8.746\n",
            "Predicted: 22.949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVn1PsWa-int"
      },
      "source": [
        "\r\n",
        "# load mnist dataset in pytorch\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision.datasets import MNIST\r\n",
        "from torchvision.transforms import Compose\r\n",
        "from torchvision.transforms import ToTensor\r\n",
        "from matplotlib import pyplot\r\n",
        "# define location to save or load the dataset\r\n",
        "path = '~/.torch/datasets/mnist'\r\n",
        "# define the transforms to apply to the data\r\n",
        "trans = Compose([ToTensor()])\r\n",
        "# download and define the datasets\r\n",
        "train = MNIST(path, train=True, download=True, transform=trans)\r\n",
        "test = MNIST(path, train=False, download=True, transform=trans)\r\n",
        "# define how to enumerate the datasets\r\n",
        "train_dl = DataLoader(train, batch_size=32, shuffle=True)\r\n",
        "test_dl = DataLoader(test, batch_size=32, shuffle=True)\r\n",
        "# get one batch of images\r\n",
        "i, (inputs, targets) = next(enumerate(train_dl))\r\n",
        "# plot some images\r\n",
        "for i in range(25):\r\n",
        "\t# define subplot\r\n",
        "\tpyplot.subplot(5, 5, i+1)\r\n",
        "\t# plot raw pixel data\r\n",
        "\tpyplot.imshow(inputs[i][0], cmap='gray')\r\n",
        "# show the figure\r\n",
        "pyplot.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNqCG9EY_gjj",
        "outputId": "2d7021a4-4953-44c0-b114-e7879454fa17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# pytorch cnn for multiclass classification\r\n",
        "from numpy import vstack\r\n",
        "from numpy import argmax\r\n",
        "from pandas import read_csv\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from torchvision.datasets import MNIST\r\n",
        "from torchvision.transforms import Compose\r\n",
        "from torchvision.transforms import ToTensor\r\n",
        "from torchvision.transforms import Normalize\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.nn import Conv2d\r\n",
        "from torch.nn import MaxPool2d\r\n",
        "from torch.nn import Linear\r\n",
        "from torch.nn import ReLU\r\n",
        "from torch.nn import Softmax\r\n",
        "from torch.nn import Module\r\n",
        "from torch.optim import SGD\r\n",
        "from torch.nn import CrossEntropyLoss\r\n",
        "from torch.nn.init import kaiming_uniform_\r\n",
        "from torch.nn.init import xavier_uniform_\r\n",
        " \r\n",
        "# model definition\r\n",
        "class CNN(Module):\r\n",
        "    # define model elements\r\n",
        "    def __init__(self, n_channels):\r\n",
        "        super(CNN, self).__init__()\r\n",
        "        # input to first hidden layer\r\n",
        "        self.hidden1 = Conv2d(n_channels, 32, (3,3))\r\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\r\n",
        "        self.act1 = ReLU()\r\n",
        "        # first pooling layer\r\n",
        "        self.pool1 = MaxPool2d((2,2), stride=(2,2))\r\n",
        "        # second hidden layer\r\n",
        "        self.hidden2 = Conv2d(32, 32, (3,3))\r\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\r\n",
        "        self.act2 = ReLU()\r\n",
        "        # second pooling layer\r\n",
        "        self.pool2 = MaxPool2d((2,2), stride=(2,2))\r\n",
        "        # fully connected layer\r\n",
        "        self.hidden3 = Linear(5*5*32, 100)\r\n",
        "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\r\n",
        "        self.act3 = ReLU()\r\n",
        "        # output layer\r\n",
        "        self.hidden4 = Linear(100, 10)\r\n",
        "        xavier_uniform_(self.hidden4.weight)\r\n",
        "        self.act4 = Softmax(dim=1)\r\n",
        " \r\n",
        "    # forward propagate input\r\n",
        "    def forward(self, X):\r\n",
        "        # input to first hidden layer\r\n",
        "        X = self.hidden1(X)\r\n",
        "        X = self.act1(X)\r\n",
        "        X = self.pool1(X)\r\n",
        "        # second hidden layer\r\n",
        "        X = self.hidden2(X)\r\n",
        "        X = self.act2(X)\r\n",
        "        X = self.pool2(X)\r\n",
        "        # flatten\r\n",
        "        X = X.view(-1, 4*4*50)\r\n",
        "        # third hidden layer\r\n",
        "        X = self.hidden3(X)\r\n",
        "        X = self.act3(X)\r\n",
        "        # output layer\r\n",
        "        X = self.hidden4(X)\r\n",
        "        X = self.act4(X)\r\n",
        "        return X\r\n",
        " \r\n",
        "# prepare the dataset\r\n",
        "def prepare_data(path):\r\n",
        "    # define standardization\r\n",
        "    trans = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\r\n",
        "    # load dataset\r\n",
        "    train = MNIST(path, train=True, download=True, transform=trans)\r\n",
        "    test = MNIST(path, train=False, download=True, transform=trans)\r\n",
        "    # prepare data loaders\r\n",
        "    train_dl = DataLoader(train, batch_size=64, shuffle=True)\r\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\r\n",
        "    return train_dl, test_dl\r\n",
        " \r\n",
        "# train the model\r\n",
        "def train_model(train_dl, model):\r\n",
        "    # define the optimization\r\n",
        "    criterion = CrossEntropyLoss()\r\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
        "    # enumerate epochs\r\n",
        "    for epoch in range(10):\r\n",
        "        # enumerate mini batches\r\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\r\n",
        "            # clear the gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "            # compute the model output\r\n",
        "            yhat = model(inputs)\r\n",
        "            # calculate loss\r\n",
        "            loss = criterion(yhat, targets)\r\n",
        "            # credit assignment\r\n",
        "            loss.backward()\r\n",
        "            # update model weights\r\n",
        "            optimizer.step()\r\n",
        " \r\n",
        "# evaluate the model\r\n",
        "def evaluate_model(test_dl, model):\r\n",
        "    predictions, actuals = list(), list()\r\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\r\n",
        "        # evaluate the model on the test set\r\n",
        "        yhat = model(inputs)\r\n",
        "        # retrieve numpy array\r\n",
        "        yhat = yhat.detach().numpy()\r\n",
        "        actual = targets.numpy()\r\n",
        "        # convert to class labels\r\n",
        "        yhat = argmax(yhat, axis=1)\r\n",
        "        # reshape for stacking\r\n",
        "        actual = actual.reshape((len(actual), 1))\r\n",
        "        yhat = yhat.reshape((len(yhat), 1))\r\n",
        "        # store\r\n",
        "        predictions.append(yhat)\r\n",
        "        actuals.append(actual)\r\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\r\n",
        "    # calculate accuracy\r\n",
        "    acc = accuracy_score(actuals, predictions)\r\n",
        "    return acc\r\n",
        " \r\n",
        "# prepare the data\r\n",
        "path = '~/.torch/datasets/mnist'\r\n",
        "train_dl, test_dl = prepare_data(path)\r\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\r\n",
        "# define the network\r\n",
        "model = CNN(1)\r\n",
        "# # train the model\r\n",
        "train_model(train_dl, model)\r\n",
        "# evaluate the model\r\n",
        "acc = evaluate_model(test_dl, model)\r\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 10000\n",
            "Accuracy: 0.989\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}